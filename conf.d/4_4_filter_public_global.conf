filter {
  ## Ensure that the [#source] field only has one element by removing a
  ## possibly existing second element with index 1.
  mutate {
    remove_field => [
      "[#source][1]",
      "#tmp_serialized_event"
    ]
  }
  mutate {
    update => {
      "#source" => "%{[#source]}"
    }
  }

  ## Also calculate fingerprint for this to keep the test framework simpler by
  ## not making an exception.
  metrics {
    meter => ["events"]
    flush_interval => 60
    rates => [1]
    clear_interval => 3600
    add_field => { "[#source]" => "elastic-logstash" }
  }


  # if ![@metadata][_id] and ("e2e-tests" in [#source] or [@metadata][test_mode]) {
  if ![@metadata][_id] {
    ## https://www.elastic.co/blog/logstash-lessons-handling-duplicates
    ## This needs to be done as early as possible against the raw event to allow
    ## changes to the post-processing without altering the ID.
    ## SHA2 is choosen because it is considered cryptographically secure (other than SHA1 which is broken).
    ## This is also important because otherwise, someone who could potentially
    ## do a second-preimage attack on MD5 to overwrite/change any log in ES that
    ## she does not like and of which she has the original version (log line).
    ## SHA512 is chosen over SHA256 because SHA512 is considered faster on modern CPUs.
    ## MD5 was considered because it is slightly faster according to `openssl speed md5 sha512`.
    ## When checking the CPU load without fingerprinting and with
    ## fingerprinting (SHA512 or MD5) no real impact is noticed.
    ## This test was done once before with an CPU load impact from 15 % (base
    ## line) to 25 % (with fingerprinting) but this test is considered wrong now.
    ## If the ID is already present in the input, use it. But in general, it is
    ## preferred to not store the ID in the archive and regenerate it.


    ## List all sources here which already ship with the correct [@timestamp] field.
    ## Meaning that [@timestamp] does not need to be extracted from the [message] field.
    ## This is important because we definitely want to include the timestamp in the hash.
    ## But if [@timestamp] is not contained in the original event, Logstash will add it at input time
    ## so it would be different on archive restore.

    ## FIXME: IDs are not deterministic when being calculated on different Logstash hosts.
    if [#source] in ["e2e-tests", "windows-eventlog", "wireless-eventlog-fastping", "curator"] {

      fingerprint {
        concatenate_all_fields => true
        ## Unix socket adds path. https://github.com/logstash-plugins/logstash-input-unix/issues/26
        exclude => [
          "#logstash_timestamp",

          "@version",
          "path",
          "port",
          "input_type",
          "type",
          "source"
        ]
        target => "[@metadata][_id]"
        method => "SHA512"
        ## The key of the HMAC can be used to proof the authenticity of the
        ## source fields as a side effect.
        ## Note that HMAC is an authentication code and not an "encryption" even though we have a "key" defined.
        key => "eMorL6N3mIPdfVeBY67oVIk35GjFHec4oB8egVuHiDwssKsklM"
        base64encode => true
        # deep_sort => true
      }

    } else {

      fingerprint {
        concatenate_all_fields => true
        exclude => [
          "#logstash_timestamp",

          "@version",
          "path",
          "port",
          "input_type",
          "type",
          "source",

          "host",
          "@timestamp"
        ]
        target => "[@metadata][_id]"
        method => "SHA512"
        key => "eMorL6N3mIPdfVeBY67oVIk35GjFHec4oB8egVuHiDwssKsklM"
        base64encode => true
        # deep_sort => true
      }

    }

    ## Truncate is done because authentication code is merely a side effect but
    ## can still be used, only with limited size.
    truncate {
      fields => ["[@metadata][_id]"]
      length_bytes => 23
    }
  }

}
